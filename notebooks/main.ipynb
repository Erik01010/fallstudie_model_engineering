{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64331896-8016-4287-bcfa-f5d2eb42f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "RAW_DATA_PATH = \"data.xlsx\"\n",
    "\n",
    "CAT_FEATURES = [\"country\", \"card\", \"PSP\"]\n",
    "\n",
    "CYCLICAL_FEATURES = {\"day\": 31, \"dow\": 7, \"hour\": 24}\n",
    "\n",
    "PSP_COSTS = {\n",
    "    \"Moneycard\": {\"success\": 5, \"failure\": 2},\n",
    "    \"Goldcard\": {\"success\": 10, \"failure\": 5},\n",
    "    \"UK_Card\": {\"success\": 3, \"failure\": 1},\n",
    "    \"Simplecard\": {\"success\": 1, \"failure\": 0.5},\n",
    "}\n",
    "\n",
    "PARAM_DIST = {\n",
    "    \"learning_rate\": uniform(0.01, 0.2),\n",
    "    \"max_iter\": randint(100, 500),\n",
    "    \"max_depth\": randint(3, 10),\n",
    "    \"l2_regularization\": uniform(0, 1),\n",
    "    \"min_samples_leaf\": randint(20, 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "078fef9b-ddcb-4928-8fff-9afdf26f0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(data: pd.DataFrame, ohc: OneHotEncoder) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and generate Features.\"\"\"\n",
    "    data = data.copy()\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Informationen aus Zeitstempel extrahieren\n",
    "    data[\"month\"] = data.loc[:, \"tmsp\"].dt.month.astype(\"int64\")\n",
    "    data[\"week\"] = data.loc[:, \"tmsp\"].dt.isocalendar().week.astype(\"int64\")\n",
    "    data[\"day\"] = data.loc[:, \"tmsp\"].dt.day.astype(\"int64\")\n",
    "    data[\"dow\"] = data.loc[:, \"tmsp\"].dt.dayofweek.astype(\"int64\")\n",
    "    data[\"hour\"] = data.loc[:, \"tmsp\"].dt.hour.astype(\"int64\")\n",
    "    data[\"second\"] = data.loc[:, \"tmsp\"].dt.second.astype(\"int64\")\n",
    "    data[\"is_weekend\"] = data[\"dow\"] >= 5\n",
    "    data[\"is_business_hours\"] = (data[\"hour\"] >= 8) & (data[\"hour\"] < 20)\n",
    "\n",
    "    # Zeit-Features zyklisch kodieren\n",
    "    # week und month nicht zyklisch kodieren da kein Zyklusübergang\n",
    "    for key, value in CYCLICAL_FEATURES.items():\n",
    "        data[f\"{key}_sin\"] = np.sin(2 * np.pi * data[key] / value)\n",
    "        data[f\"{key}_cos\"] = np.cos(2 * np.pi * data[key] / value)\n",
    "\n",
    "    # Kosten\n",
    "    data[\"cost_if_success\"] = data[\"PSP\"].map(lambda psp: PSP_COSTS[psp][\"success\"])\n",
    "    data[\"cost_if_failure\"] = data[\"PSP\"].map(lambda psp: PSP_COSTS[psp][\"failure\"])\n",
    "\n",
    "    # Wiederholte Transaktionsversuche aufgrund fehlgeschlagener Transaktionen\n",
    "    data[\"timedelta\"] = data[\"tmsp\"].diff().dt.total_seconds().fillna(0).astype(\"int64\")\n",
    "    cols_to_compare = [\"country\", \"amount\", \"3D_secured\", \"card\"]\n",
    "    data[\"is_retry\"] = (data[cols_to_compare] == data[cols_to_compare].shift(1)).all(\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Anzahl kontinuierlicher Retry Versuche\n",
    "    retry_groups = (~data[\"is_retry\"]).cumsum()\n",
    "    data[\"retry_count\"] = (\n",
    "        data.groupby(retry_groups)[\"is_retry\"].cumsum().astype(\"int64\")\n",
    "    )\n",
    "\n",
    "    # Wechsel PSP bei Retry\n",
    "    data[\"PSP_switch\"] = data.groupby(retry_groups)[\"PSP\"].transform(\n",
    "        lambda x: (x != x.shift()).fillna(False).cumsum() > 0\n",
    "    )\n",
    "\n",
    "    # kategorische Merkmale encodieren\n",
    "    encoded_array = ohc.transform(data[CAT_FEATURES])\n",
    "    encoded_columns = ohc.get_feature_names_out(CAT_FEATURES)\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoded_columns, index=data.index)\n",
    "    data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "    # Timestamp und nicht kategorische features entfernen\n",
    "    data = data.drop(columns=[\"tmsp\", \"PSP\", \"country\", \"card\"], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1899489d-c37e-4206-a029-5b5f17865882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ohc_encoder(data: pd.DataFrame) -> OneHotEncoder:\n",
    "    \"\"\"Trains and saves a OneHotEncoder for categorical features.\"\"\"\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_encoder.fit_transform(data)\n",
    "    return one_hot_encoder\n",
    "\n",
    "\n",
    "def train_decision_tree(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> DecisionTreeClassifier:\n",
    "    \"\"\"Trains a Decision Tree Classifier.\"\"\"\n",
    "    decision_tree_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    decision_tree_model.fit(x_train, y_train)\n",
    "\n",
    "    return decision_tree_model\n",
    "\n",
    "\n",
    "def train_hgboost(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> HistGradientBoostingClassifier:\n",
    "    \"\"\"Trains a HGBoost Classifier.\"\"\"\n",
    "    hgboost_model = HistGradientBoostingClassifier(\n",
    "        max_leaf_nodes=30, learning_rate=0.05, random_state=42\n",
    "    )\n",
    "    hgboost_model.fit(x_train, y_train)\n",
    "\n",
    "    return hgboost_model\n",
    "\n",
    "\n",
    "def calculate_success_probability(model, features: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates the success probability for a given model and features.\"\"\"\n",
    "    return model.predict_proba(features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eb97192-fa46-4da8-ac0d-01e6c6a0059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_technical_performance(\n",
    "    model, x_test: pd.DataFrame, y_test: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"Evaluates the model on the test set by calculating accuracy.\"\"\"\n",
    "    y_pred_proba = calculate_success_probability(model, x_test)\n",
    "    score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"ROC AUC Score: {score:.4f}\")\n",
    "\n",
    "\n",
    "def _calculate_actual_costs(choices, y_true) -> float:\n",
    "    \"\"\"\n",
    "    Helper function to calculate the total actual cost for a series of PSP choices\n",
    "    based on the true transaction outcomes.\n",
    "    \"\"\"\n",
    "    total_cost = 0\n",
    "\n",
    "    for index, psp_choice in choices.items():\n",
    "        if psp_choice in PSP_COSTS:\n",
    "            cost_dict = PSP_COSTS[psp_choice]\n",
    "            # Use the true outcome (y_true) to determine the actual cost\n",
    "            actual_cost = (\n",
    "                cost_dict[\"success\"] if y_true.loc[index] else cost_dict[\"failure\"]\n",
    "            )\n",
    "            total_cost += actual_cost\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def evaluate_business_impact(\n",
    "    model, x_test: pd.DataFrame, y_test: pd.DataFrame, original_data: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"Evaluates and compares the financial outcome of the model's routing strategy\n",
    "    against the legacy system's strategy on the test set.\"\"\"\n",
    "    all_model_columns = x_test.columns.tolist()\n",
    "    expected_costs_df = pd.DataFrame(index=x_test.index)\n",
    "\n",
    "    for psp in PSP_COSTS:\n",
    "        simulated_features = x_test.copy()\n",
    "\n",
    "        for col in all_model_columns:\n",
    "            if col.startswith(\"PSP_\"):\n",
    "                simulated_features[col] = 0\n",
    "        simulated_features[f\"PSP_{psp}\"] = 1\n",
    "        simulated_features = simulated_features.reindex(\n",
    "            columns=all_model_columns, fill_value=0\n",
    "        )\n",
    "        prob_success = calculate_success_probability(model, simulated_features)\n",
    "\n",
    "        expected_costs_df[psp] = (\n",
    "            prob_success * PSP_COSTS[psp][\"success\"]\n",
    "            + (1 - prob_success) * PSP_COSTS[psp][\"failure\"]\n",
    "        )\n",
    "\n",
    "    # Calculate Model Strategy Cost\n",
    "    model_choices = expected_costs_df.idxmin(axis=1)\n",
    "    total_cost_model = _calculate_actual_costs(model_choices, y_test)\n",
    "\n",
    "    # Calculate Legacy System Cost\n",
    "    legacy_choices = original_data.loc[x_test.index, \"PSP\"]\n",
    "    total_cost_legacy = _calculate_actual_costs(legacy_choices, y_test)\n",
    "\n",
    "    # Report the Financial Outcome\n",
    "    savings = total_cost_legacy - total_cost_model\n",
    "    savings_percent = (\n",
    "        (savings / total_cost_legacy) * 100 if total_cost_legacy > 0 else 0\n",
    "    )\n",
    "\n",
    "    print(f\"  Legacy System Cost: {total_cost_legacy:,.2f} €\")\n",
    "    print(f\"  Model Strategy Cost: {total_cost_model:,.2f} €\")\n",
    "    print(f\"  Savings: {savings:,.2f} € ({savings_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca7f1523-88b3-4320-b273-5680dc308edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hgb_model(\n",
    "    x_train: pd.DataFrame, y_train: pd.DataFrame\n",
    ") -> HistGradientBoostingClassifier:\n",
    "    \"\"\"Run randomized search to find the best hyperparameters.\"\"\"\n",
    "    hgboost_model = HistGradientBoostingClassifier(random_state=42)\n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=hgboost_model,\n",
    "        param_distributions=PARAM_DIST,\n",
    "        n_iter=1,\n",
    "        cv=cv_strategy,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    random_search.fit(x_train, y_train)\n",
    "\n",
    "    print(f\"Beste Parameter gefunden: {random_search.best_params_}\")\n",
    "    print(f\"Bester CV AUC-Score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "    final_model = HistGradientBoostingClassifier(\n",
    "        **random_search.best_params_, random_state=42\n",
    "    )\n",
    "    final_model.fit(x_train, y_train)\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3622bb41-d06d-45f6-856e-5c25e0d50748",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_excel(RAW_DATA_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91785b62-d91f-461e-842b-71ce80b2a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohc = train_ohc_encoder(data=raw_data[CAT_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93149e2f-858a-4d4f-928f-356c1776ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = engineer_features(data=raw_data, ohc=ohc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "740126db-792b-46e8-91db-3a249397b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = processed_data[\"success\"]\n",
    "X = processed_data.drop(columns=[\"success\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b303bf2c-1ff1-4755-8171-36b154463415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter gefunden: {'l2_regularization': 0.3745401188473625, 'learning_rate': 0.20014286128198325, 'max_depth': 5, 'max_iter': 171, 'min_samples_leaf': 80}\n",
      "Bester CV AUC-Score: 0.6750\n",
      "\n",
      "--- Model Evaluation ---\n",
      "\n",
      "Evaluating decision_tree_model\n",
      "ROC AUC Score: 0.6600\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "Evaluating hgboost_model\n",
      "ROC AUC Score: 0.6790\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "Evaluating optimized_hgboost_model\n",
      "ROC AUC Score: 0.6757\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "--- Evaluation complete ---\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "dcm = train_decision_tree(x_train=X_train, y_train=y_train)\n",
    "hgbm = train_hgboost(x_train=X_train, y_train=y_train)\n",
    "oghbm = find_best_hgb_model(x_train=X_train, y_train=y_train)\n",
    "models_to_evaluate = {\n",
    "    \"decision_tree_model\": dcm,\n",
    "    \"hgboost_model\": hgbm,\n",
    "    \"optimized_hgboost_model\": oghbm,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "for name, model in models_to_evaluate.items():\n",
    "    print(f\"\\nEvaluating {name}\")\n",
    "    evaluate_technical_performance(model=model, x_test=X_test, y_test=y_test)\n",
    "    evaluate_business_impact(\n",
    "        model=model, x_test=X_test, y_test=y_test, original_data=raw_data\n",
    "    )\n",
    "print(\"\\n--- Evaluation complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551fd71-23a5-4fab-aab5-1716549707f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f1f7e60-1242-4820-b0cb-5a0b95a13c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation ---\n",
      "ROC AUC Score: 0.6600\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "--- Evaluation complete ---\n",
      "\n",
      "--- Model Evaluation ---\n",
      "ROC AUC Score: 0.6790\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "--- Evaluation complete ---\n",
      "\n",
      "--- Model Evaluation ---\n",
      "Beste Parameter gefunden: {'l2_regularization': 0.3745401188473625, 'learning_rate': 0.20014286128198325, 'max_depth': 5, 'max_iter': 171, 'min_samples_leaf': 80}\n",
      "Bester CV AUC-Score: 0.6750\n",
      "ROC AUC Score: 0.6757\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "--- Evaluation complete ---\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "dcm = train_decision_tree(x_train=X_train, y_train=y_train)\n",
    "hgbm = train_hgboost(x_train=X_train, y_train=y_train)\n",
    "models_to_evaluate = {\n",
    "    \"decision_tree_model\": dcm,\n",
    "    \"hgboost_model\": hgbm,\n",
    "    \"optimized_hgboost_model\": oghbm,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "dcm = train_decision_tree(x_train=X_train, y_train=y_train)\n",
    "evaluate_technical_performance(model=dcm, x_test=X_test, y_test=y_test)\n",
    "evaluate_business_impact(\n",
    "    model=dcm, x_test=X_test, y_test=y_test, original_data=raw_data\n",
    ")\n",
    "print(\"\\n--- Evaluation complete ---\")\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "hgbm = train_hgboost(x_train=X_train, y_train=y_train)\n",
    "evaluate_technical_performance(model=hgbm, x_test=X_test, y_test=y_test)\n",
    "evaluate_business_impact(\n",
    "    model=hgbm, x_test=X_test, y_test=y_test, original_data=raw_data\n",
    ")\n",
    "print(\"\\n--- Evaluation complete ---\")\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "oghbm = find_best_hgb_model(x_train=X_train, y_train=y_train)\n",
    "evaluate_technical_performance(model=oghbm, x_test=X_test, y_test=y_test)\n",
    "evaluate_business_impact(\n",
    "    model=oghbm, x_test=X_test, y_test=y_test, original_data=raw_data\n",
    ")\n",
    "print(\"\\n--- Evaluation complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50bf47b7-376a-453d-8f77-d9d5d65de680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter gefunden: {'l2_regularization': 0.3745401188473625, 'learning_rate': 0.20014286128198325, 'max_depth': 5, 'max_iter': 171, 'min_samples_leaf': 80}\n",
      "Bester CV AUC-Score: 0.6750\n",
      "\n",
      "--- Model Evaluation ---\n",
      "\n",
      "Evaluating decision_tree_model\n",
      "ROC AUC Score: 0.6600\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "Evaluating hgboost_model\n",
      "ROC AUC Score: 0.6790\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "Evaluating optimized_hgboost_model\n",
      "ROC AUC Score: 0.6757\n",
      "  Legacy System Cost: 17,927.00 €\n",
      "  Model Strategy Cost: 6,074.00 €\n",
      "  Savings: 11,853.00 € (66.12%)\n",
      "\n",
      "--- Evaluation complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "RAW_DATA_PATH = \"data.xlsx\"\n",
    "\n",
    "CAT_FEATURES = [\"country\", \"card\", \"PSP\"]\n",
    "\n",
    "CYCLICAL_FEATURES = {\"day\": 31, \"dow\": 7, \"hour\": 24}\n",
    "\n",
    "PSP_COSTS = {\n",
    "    \"Moneycard\": {\"success\": 5, \"failure\": 2},\n",
    "    \"Goldcard\": {\"success\": 10, \"failure\": 5},\n",
    "    \"UK_Card\": {\"success\": 3, \"failure\": 1},\n",
    "    \"Simplecard\": {\"success\": 1, \"failure\": 0.5},\n",
    "}\n",
    "\n",
    "PARAM_DIST = {\n",
    "    \"learning_rate\": uniform(0.01, 0.2),\n",
    "    \"max_iter\": randint(100, 500),\n",
    "    \"max_depth\": randint(3, 10),\n",
    "    \"l2_regularization\": uniform(0, 1),\n",
    "    \"min_samples_leaf\": randint(20, 100),\n",
    "}\n",
    "\n",
    "\n",
    "def engineer_features(data: pd.DataFrame, ohc: OneHotEncoder) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and generate Features.\"\"\"\n",
    "    data = data.copy()\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Informationen aus Zeitstempel extrahieren\n",
    "    data[\"month\"] = data.loc[:, \"tmsp\"].dt.month.astype(\"int64\")\n",
    "    data[\"week\"] = data.loc[:, \"tmsp\"].dt.isocalendar().week.astype(\"int64\")\n",
    "    data[\"day\"] = data.loc[:, \"tmsp\"].dt.day.astype(\"int64\")\n",
    "    data[\"dow\"] = data.loc[:, \"tmsp\"].dt.dayofweek.astype(\"int64\")\n",
    "    data[\"hour\"] = data.loc[:, \"tmsp\"].dt.hour.astype(\"int64\")\n",
    "    data[\"second\"] = data.loc[:, \"tmsp\"].dt.second.astype(\"int64\")\n",
    "    data[\"is_weekend\"] = data[\"dow\"] >= 5\n",
    "    data[\"is_business_hours\"] = (data[\"hour\"] >= 8) & (data[\"hour\"] < 20)\n",
    "\n",
    "    # Zeit-Features zyklisch kodieren\n",
    "    # week und month nicht zyklisch kodieren da kein Zyklusübergang\n",
    "    for key, value in CYCLICAL_FEATURES.items():\n",
    "        data[f\"{key}_sin\"] = np.sin(2 * np.pi * data[key] / value)\n",
    "        data[f\"{key}_cos\"] = np.cos(2 * np.pi * data[key] / value)\n",
    "\n",
    "    # Kosten\n",
    "    data[\"cost_if_success\"] = data[\"PSP\"].map(lambda psp: PSP_COSTS[psp][\"success\"])\n",
    "    data[\"cost_if_failure\"] = data[\"PSP\"].map(lambda psp: PSP_COSTS[psp][\"failure\"])\n",
    "\n",
    "    # Wiederholte Transaktionsversuche aufgrund fehlgeschlagener Transaktionen\n",
    "    data[\"timedelta\"] = data[\"tmsp\"].diff().dt.total_seconds().fillna(0).astype(\"int64\")\n",
    "    cols_to_compare = [\"country\", \"amount\", \"3D_secured\", \"card\"]\n",
    "    data[\"is_retry\"] = (data[cols_to_compare] == data[cols_to_compare].shift(1)).all(\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Anzahl kontinuierlicher Retry Versuche\n",
    "    retry_groups = (~data[\"is_retry\"]).cumsum()\n",
    "    data[\"retry_count\"] = (\n",
    "        data.groupby(retry_groups)[\"is_retry\"].cumsum().astype(\"int64\")\n",
    "    )\n",
    "\n",
    "    # Wechsel PSP bei Retry\n",
    "    data[\"PSP_switch\"] = data.groupby(retry_groups)[\"PSP\"].transform(\n",
    "        lambda x: (x != x.shift()).fillna(False).cumsum() > 0\n",
    "    )\n",
    "\n",
    "    # kategorische Merkmale encodieren\n",
    "    encoded_array = ohc.transform(data[CAT_FEATURES])\n",
    "    encoded_columns = ohc.get_feature_names_out(CAT_FEATURES)\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoded_columns, index=data.index)\n",
    "    data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "    # Timestamp und nicht kategorische features entfernen\n",
    "    data = data.drop(columns=[\"tmsp\", \"PSP\", \"country\", \"card\"], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_ohc_encoder(data: pd.DataFrame) -> OneHotEncoder:\n",
    "    \"\"\"Trains and saves a OneHotEncoder for categorical features.\"\"\"\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_encoder.fit_transform(data)\n",
    "    return one_hot_encoder\n",
    "\n",
    "\n",
    "def train_decision_tree(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> DecisionTreeClassifier:\n",
    "    \"\"\"Trains a Decision Tree Classifier.\"\"\"\n",
    "    decision_tree_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    decision_tree_model.fit(x_train, y_train)\n",
    "\n",
    "    return decision_tree_model\n",
    "\n",
    "\n",
    "def train_hgboost(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> HistGradientBoostingClassifier:\n",
    "    \"\"\"Trains a HGBoost Classifier.\"\"\"\n",
    "    hgboost_model = HistGradientBoostingClassifier(\n",
    "        max_leaf_nodes=30, learning_rate=0.05, random_state=42\n",
    "    )\n",
    "    hgboost_model.fit(x_train, y_train)\n",
    "\n",
    "    return hgboost_model\n",
    "\n",
    "\n",
    "def calculate_success_probability(model, features: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates the success probability for a given model and features.\"\"\"\n",
    "    return model.predict_proba(features)[:, 1]\n",
    "\n",
    "\n",
    "def evaluate_technical_performance(\n",
    "    model, x_test: pd.DataFrame, y_test: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"Evaluates the model on the test set by calculating accuracy.\"\"\"\n",
    "    y_pred_proba = calculate_success_probability(model, x_test)\n",
    "    score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"ROC AUC Score: {score:.4f}\")\n",
    "\n",
    "\n",
    "def _calculate_actual_costs(choices, y_true) -> float:\n",
    "    \"\"\"\n",
    "    Helper function to calculate the total actual cost for a series of PSP choices\n",
    "    based on the true transaction outcomes.\n",
    "    \"\"\"\n",
    "    total_cost = 0\n",
    "\n",
    "    for index, psp_choice in choices.items():\n",
    "        if psp_choice in PSP_COSTS:\n",
    "            cost_dict = PSP_COSTS[psp_choice]\n",
    "            # Use the true outcome (y_true) to determine the actual cost\n",
    "            actual_cost = (\n",
    "                cost_dict[\"success\"] if y_true.loc[index] else cost_dict[\"failure\"]\n",
    "            )\n",
    "            total_cost += actual_cost\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def evaluate_business_impact(\n",
    "    model, x_test: pd.DataFrame, y_test: pd.DataFrame, original_data: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"Evaluates and compares the financial outcome of the model's routing strategy\n",
    "    against the legacy system's strategy on the test set.\"\"\"\n",
    "    all_model_columns = x_test.columns.tolist()\n",
    "    expected_costs_df = pd.DataFrame(index=x_test.index)\n",
    "\n",
    "    for psp in PSP_COSTS:\n",
    "        simulated_features = x_test.copy()\n",
    "\n",
    "        for col in all_model_columns:\n",
    "            if col.startswith(\"PSP_\"):\n",
    "                simulated_features[col] = 0\n",
    "        simulated_features[f\"PSP_{psp}\"] = 1\n",
    "        simulated_features = simulated_features.reindex(\n",
    "            columns=all_model_columns, fill_value=0\n",
    "        )\n",
    "        prob_success = calculate_success_probability(model, simulated_features)\n",
    "\n",
    "        expected_costs_df[psp] = (\n",
    "            prob_success * PSP_COSTS[psp][\"success\"]\n",
    "            + (1 - prob_success) * PSP_COSTS[psp][\"failure\"]\n",
    "        )\n",
    "\n",
    "    # Calculate Model Strategy Cost\n",
    "    model_choices = expected_costs_df.idxmin(axis=1)\n",
    "    total_cost_model = _calculate_actual_costs(model_choices, y_test)\n",
    "\n",
    "    # Calculate Legacy System Cost\n",
    "    legacy_choices = original_data.loc[x_test.index, \"PSP\"]\n",
    "    total_cost_legacy = _calculate_actual_costs(legacy_choices, y_test)\n",
    "\n",
    "    # Report the Financial Outcome\n",
    "    savings = total_cost_legacy - total_cost_model\n",
    "    savings_percent = (\n",
    "        (savings / total_cost_legacy) * 100 if total_cost_legacy > 0 else 0\n",
    "    )\n",
    "\n",
    "    print(f\"  Legacy System Cost: {total_cost_legacy:,.2f} €\")\n",
    "    print(f\"  Model Strategy Cost: {total_cost_model:,.2f} €\")\n",
    "    print(f\"  Savings: {savings:,.2f} € ({savings_percent:.2f}%)\")\n",
    "\n",
    "\n",
    "def find_best_hgb_model(\n",
    "    x_train: pd.DataFrame, y_train: pd.DataFrame\n",
    ") -> HistGradientBoostingClassifier:\n",
    "    \"\"\"Run randomized search to find the best hyperparameters.\"\"\"\n",
    "    hgboost_model = HistGradientBoostingClassifier(random_state=42)\n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=hgboost_model,\n",
    "        param_distributions=PARAM_DIST,\n",
    "        n_iter=1,\n",
    "        cv=cv_strategy,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    random_search.fit(x_train, y_train)\n",
    "\n",
    "    print(f\"Beste Parameter gefunden: {random_search.best_params_}\")\n",
    "    print(f\"Bester CV AUC-Score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "    final_model = HistGradientBoostingClassifier(\n",
    "        **random_search.best_params_, random_state=42\n",
    "    )\n",
    "    final_model.fit(x_train, y_train)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "\n",
    "raw_data = pd.read_excel(RAW_DATA_PATH, index_col=0)\n",
    "ohc = train_ohc_encoder(data=raw_data[CAT_FEATURES])\n",
    "processed_data = engineer_features(data=raw_data, ohc=ohc)\n",
    "y = processed_data[\"success\"]\n",
    "X = processed_data.drop(columns=[\"success\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# Train models\n",
    "dcm = train_decision_tree(x_train=X_train, y_train=y_train)\n",
    "hgbm = train_hgboost(x_train=X_train, y_train=y_train)\n",
    "oghbm = find_best_hgb_model(x_train=X_train, y_train=y_train)\n",
    "models_to_evaluate = {\n",
    "    \"decision_tree_model\": dcm,\n",
    "    \"hgboost_model\": hgbm,\n",
    "    \"optimized_hgboost_model\": oghbm,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "for name, model in models_to_evaluate.items():\n",
    "    print(f\"\\nEvaluating {name}\")\n",
    "    evaluate_technical_performance(model=model, x_test=X_test, y_test=y_test)\n",
    "    evaluate_business_impact(\n",
    "        model=model, x_test=X_test, y_test=y_test, original_data=raw_data\n",
    "    )\n",
    "print(\"\\n--- Evaluation complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a631df9-69c3-4b6d-a213-8d8757575f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
